Here I will be keeping all the understanding I will be getting from the project.

------------------Docker Commands:
docker ps : Shows all the running containers
docker ps -a : Shows all the containers along with the images attached to them
docker rm [containerId]: Removes the given container
docker stop [containerId]: Stops the given container
docker start [containerId]: Starts it
docker rmi [imageId]: Removes the given image
docker run [imageName]: runs the image or pulls the image to local from dockerHub

docker build: docker build -t [app name] .
ex. docker build -t minimal-node-app .

Run docker : docker run -p yourport:yourport [app name]
ex: docker run -p 3000:3000 minimal-node-app

--------------------

Use Case	Benefit
docker inspect	Shows which ports are intended to be exposed
docker-compose	Reads this as a hint when configuring services
Documentation	Tells others what ports your app expects

--------------Dockerfile:

EXPOSE 3000 does NOT do any actual port mapping. 
It's for Informational only

A way to document which port the app inside the container listens on

Helpful when using tools like docker-compose, or when other developers read your Dockerfile

----------------Sample of a docker file:
# Use an official Node.js image
FROM node:18

# Set working directory inside the container
WORKDIR /app

# Copy package.json and install dependencies
COPY package*.json ./
RUN npm install

# Copy the rest of your app‚Äôs code
COPY . .

# Expose the port your app runs on
EXPOSE 3000

# Command to run your app
CMD [ "node", "index.js" ]


-------------------Why do we copy package first and then install npm and then copy everything from our folder to /app directory in container?
Ans: 
It‚Äôs all about Docker layer caching:
Docker builds images in layers, and it caches each step unless something changes. That means:

If you change a file that comes after COPY package*.json ./, Docker will reuse the earlier cached layers and not re-run npm install (which saves time).

If you copied the entire app first (COPY . .) before npm install, then any code change, even a small one, would break the cache and force a full reinstall of dependencies.


----------------Sample example:

In the current structure if you:

Change a JS file ‚Üí only COPY . . and later steps run. npm install is cached 

Change package.json ‚Üí cache is invalidated, and Docker runs npm install again 

If you only do: COPY . . and then npm install :

Then any change in your codebase, even a single console.log, would force Docker to re-run npm install every time ‚Äî slower, inefficient.

---------------------------------------------------------
Docker Compose: Run multiple containers 

Sample of a docker compose.yml file

version: '3.8'

services:
  app:
    build: .
    ports:
      - "3000:3000"
    depends_on:
      - mongo

  mongo:
    image: mongo
    ports:
      - "27017:27017"
    volumes:
      - mongo-data:/data/db

volumes:
  mongo-data:

> depends_on:

Declares a dependency on the mongo service, meaning Docker Compose will start mongo before app.
 Note: This does not wait for MongoDB to be "ready to accept connections" ‚Äî only that the container has started. You'll still want retry logic or proper error handling in your app.


docker-compose up --build

docker-compose up: will start up the multiple containers assigned in the docker-compose.yml file
docker-compose build: Will build the specified images 



--------------------------------
You can run your Node.js app and MongoDB without Docker.

You could:

Run MongoDB locally on your machine.

Use npm start to run your Node app.

Connect to MongoDB via localhost and everything would still work.

But Docker brings a lot of important benefits in real-world scenarios ‚Äî especially in DevOps, cloud, and production environments.

üöÄ Why Docker Is Significant Here
1. Environment Consistency
‚ÄúIt works on my machine!‚Äù ‚Äî not a valid excuse anymore.

Docker ensures that your app always runs in the same environment, regardless of your OS or dependencies.

No more system-specific bugs, missing Node versions, or MongoDB setup issues.

2. Quick Setup & Teardown
With docker-compose up, you can start your whole app with one command.

No need to install Mongo, Node, or anything else manually.

3. Isolation
Your Node app and MongoDB run in separate containers, isolated from your system.

They don‚Äôt interfere with other versions or processes running on your machine.

4. Portability & Sharing
You can package your app and share it with anyone or deploy it to any server ‚Äî just with your Dockerfile and docker-compose.yml.

No setup headaches.

5. Infrastructure as Code
Your entire app setup ‚Äî including MongoDB ‚Äî is now version-controlled and reproducible.

This is a best practice in DevOps and CI/CD.

6. Easier CI/CD & Cloud Deployment
Most cloud platforms, like AWS, GCP, Azure, and Kubernetes, expect apps to be containerized.

Your app is now ready for production pipelines.

üß† Quick Analogy
Think of Docker like a shipping container for your software.

Without Docker: You hand someone a loose pile of software, hoping they install it the right way.

With Docker: You hand them a sealed, self-contained unit that works the same on every ship, truck, or platform.


